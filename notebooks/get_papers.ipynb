{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os  \n",
    "from typing import Literal, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import stamina\n",
    "import structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query term, e.g., 'natural language processing'\")\n",
    "    venue: Literal[\"ACL Anthology\"] = Field(\"ACL Anthology\", description=\"Publication venue to filter papers from\")\n",
    "    num_papers: int = Field(10, gt=0, description=\"Number of papers to retrieve\")\n",
    "    output_dir: str = Field(\"acl_papers\", description=\"Directory to save downloaded PDFs\")\n",
    "    api_key: str | None = Field(os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\"), description=\"API key for Semantic Scholar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_acl_papers(query: str, max_results: int = 100) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches ACL papers based on a query, using pagination to retrieve more results if available.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search term to query papers.\n",
    "    - max_results (int): The maximum number of papers to fetch in total.\n",
    "    \n",
    "    Returns:\n",
    "    - dict[str, dict[str, Any]]: A dictionary with ACL IDs as keys and dictionaries containing\n",
    "      the title and paperId as values.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"fields\": \"paperId,title,externalIds,openAccessPdf\",\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
    "\n",
    "    acl_papers = {}\n",
    "    total_fetched = 0\n",
    "    token = None\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        if token:\n",
    "            params[\"token\"] = token\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Process and store ACL papers\n",
    "            for paper in data.get(\"data\", []):\n",
    "                external_ids = paper.get(\"externalIds\", {})\n",
    "                if \"ACL\" in external_ids:\n",
    "                    acl_id = external_ids[\"ACL\"]\n",
    "                    acl_papers[acl_id] = {\n",
    "                        \"title\": paper.get(\"title\"),\n",
    "                        \"paperId\": paper.get(\"paperId\"),\n",
    "                        \"openAccessPdf\": paper.get(\"openAccessPdf\", {}).get(\"url\") if paper.get(\"openAccessPdf\") else None\n",
    "                    }\n",
    "                    total_fetched += 1\n",
    "                    if total_fetched >= max_results:\n",
    "                        break\n",
    "\n",
    "            # Get the next token for pagination, or break if no more pages\n",
    "            token = data.get(\"token\")\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"Retrieved {len(acl_papers)} ACL papers.\")\n",
    "    return acl_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-31 21:29:54\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRetrieved 20 ACL papers.      \u001b[0m\n",
      "ACL ID: P14-5010, Title: The Stanford CoreNLP Natural Language Processing Toolkit, Paper ID: 2f5102ec3f70d0dea98c957cc2cab4d15d83a2da\n",
      "ACL ID: J00-2011, Title: Book Reviews: Foundations of Statistical Natural Language Processing, Paper ID: 084c55d6432265785e3ff86a2e900a49d501c00a\n",
      "ACL ID: 2020.acl-demos.14, Title: Stanza: A Python Natural Language Processing Toolkit for Many Human Languages, Paper ID: 641a9749fe546a02bbab9a86bfc91492db1c3bc5\n",
      "ACL ID: 2023.nlposs-1.4, Title: PyThaiNLP: Thai Natural Language Processing in Python, Paper ID: 17fd7b820b0879734a2c08c20a890ddc526cd83d\n",
      "ACL ID: D15-1075, Title: A large annotated corpus for learning natural language inference, Paper ID: f04df4e20a18358ea2f689b4c129781628ef7fc1\n",
      "ACL ID: J96-1002, Title: A Maximum Entropy Approach to Natural Language Processing, Paper ID: fb486e03369a64de2d5b0df86ec0a7b55d3907db\n",
      "ACL ID: J00-4006, Title: Book Reviews: Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, Paper ID: 1254c99a381d1f9d6abb23c74be060e4e7876985\n",
      "ACL ID: W18-2501, Title: AllenNLP: A Deep Semantic Natural Language Processing Platform, Paper ID: 93b4cc549a1bc4bc112189da36c318193d05d806\n",
      "ACL ID: W19-5034, Title: ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing, Paper ID: de28ec1d7bd38c8fc4e8ac59b6133800818b4e29\n",
      "ACL ID: 2020.findings-emnlp.58, Title: Revisiting Pre-Trained Models for Chinese Natural Language Processing, Paper ID: d16ab5c19ed33a263b6412ac41a4ea1f068d254a\n",
      "ACL ID: 2020.acl-main.686, Title: HAT: Hardware-Aware Transformers for Efficient Natural Language Processing, Paper ID: ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7\n",
      "ACL ID: 2022.emnlp-main.414, Title: A Survey of Active Learning for Natural Language Processing, Paper ID: 3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f\n",
      "ACL ID: W19-5006, Title: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets, Paper ID: 347bac45298f37cd83c3e79d99b826dc65a70c46\n",
      "ACL ID: 2021.emnlp-demo.21, Title: Datasets: A Community Library for Natural Language Processing, Paper ID: cddf40e579a596d0110b260313adf43470617c4c\n",
      "ACL ID: W17-1101, Title: A Survey on Hate Speech Detection using Natural Language Processing, Paper ID: 6661802f5e1ee004c20a28dcce9b582d4b5fe6d7\n",
      "ACL ID: Q18-1041, Title: Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science, Paper ID: 97bfa89addc6e5d76361e4c1e296949cad887b86\n",
      "ACL ID: P19-1159, Title: Mitigating Gender Bias in Natural Language Processing: Literature Review, Paper ID: 493fac37cea49afb98c52c2f5dd75c303a325b25\n",
      "ACL ID: 2020.aacl-main.46, Title: A Survey of the State of Explainable AI for Natural Language Processing, Paper ID: 4b322cf280f459deb6d9e2eb2430d1a28141934c\n",
      "ACL ID: 2021.eacl-demos.10, Title: Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing, Paper ID: b53c386b7c65af80905dc05a9b27e98e03324739\n",
      "ACL ID: 2021.naacl-main.201, Title: A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios, Paper ID: 455cdafd55a5b5ddefa029bf97801327e142646d\n"
     ]
    }
   ],
   "source": [
    "acl_papers = fetch_acl_papers(query=\"natural language processing\", max_results=20)\n",
    "for acl_id, details in acl_papers.items():\n",
    "    print(f\"ACL ID: {acl_id}, Title: {details['title']}, Paper ID: {details['paperId']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@stamina.retry(on=requests.exceptions.RequestException, attempts=3)\n",
    "def download_pdf(pdf_url: str, pdf_filename: str) -> bool:\n",
    "    \"\"\"\n",
    "    Attempts to download a PDF file from the given URL and save it to the specified filename.\n",
    "    Retries up to 3 times on network-related exceptions.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_url (str): The URL of the PDF to download.\n",
    "    - pdf_filename (str): The path where the PDF will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if download is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(pdf_filename, \"wb\") as pdf_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            pdf_file.write(chunk)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pdfs_from_acl_id(acl_papers: dict[str, dict[str, Any]], output_dir: str = \"acl_papers\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads PDFs for the given ACL papers if a PDF URL is available, with retry capability,\n",
    "    and provides a summary of results.\n",
    "\n",
    "    Parameters:\n",
    "    - acl_papers (Dict[str, Dict[str, Any]]): Dictionary of ACL papers with ACL IDs as keys and metadata as values.\n",
    "    - output_dir (str): Directory to save downloaded PDFs.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Track successful and failed downloads\n",
    "    fetched_count = 0\n",
    "    unable_to_fetch = []\n",
    "\n",
    "    for acl_id, details in acl_papers.items():\n",
    "        pdf_url = details.get(\"openAccessPdf\")\n",
    "        \n",
    "        if pdf_url:\n",
    "            pdf_filename = os.path.join(output_dir, f\"{acl_id}.pdf\")\n",
    "            try:\n",
    "                success = download_pdf(pdf_url, pdf_filename)\n",
    "                if success:\n",
    "                    logger.info(f\"Downloaded PDF for {acl_id}: {details['title']}\")\n",
    "                    fetched_count += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Failed to download PDF for {acl_id}: {details['title']} after 3 attempts. Error: {e}\")\n",
    "                unable_to_fetch.append(f\"{acl_id}: {details['title']}\")\n",
    "        else:\n",
    "            logger.error(f\"No PDF available for {acl_id}: {details['title']}\")\n",
    "            unable_to_fetch.append(f\"{acl_id}: {details['title']}\")\n",
    "\n",
    "    # Summary of download results\n",
    "    logger.info(f\"\\nSummary:\\nFetched {fetched_count} PDFs.\\n\")\n",
    "    logger.warning(f\"Unable to fetch {len(unable_to_fetch)} PDFs:\\n\" + \"\\n\".join(unable_to_fetch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-31 21:30:38\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRetrieved 20 ACL papers.      \u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:38\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for P14-5010: The Stanford CoreNLP Natural Language Processing Toolkit\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:38\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mNo PDF available for J00-2011: Book Reviews: Foundations of Statistical Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:38\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.acl-demos.14.pdf'\", \"'./acl_papers/2020.acl-demos.14.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-demos.14.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.45\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:39\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.acl-demos.14.pdf'\", \"'./acl_papers/2020.acl-demos.14.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-demos.14.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m1.07\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.45\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:40\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for 2020.acl-demos.14: Stanza: A Python Natural Language Processing Toolkit for Many Human Languages after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-demos.14.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2023.nlposs-1.4: PyThaiNLP: Thai Natural Language Processing in Python\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:41\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://doi.org/10.18653/v1/d15-1075'\", \"'./acl_papers/D15-1075.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: http://aclweb.org/anthology/D15-1075')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.89\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:42\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://doi.org/10.18653/v1/d15-1075'\", \"'./acl_papers/D15-1075.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: http://aclweb.org/anthology/D15-1075')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.64\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.89\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:43\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for D15-1075: A large annotated corpus for learning natural language inference after 3 attempts. Error: 406 Client Error: Not Acceptable for url: http://aclweb.org/anthology/D15-1075\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:43\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mNo PDF available for J96-1002: A Maximum Entropy Approach to Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:43\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mNo PDF available for J00-4006: Book Reviews: Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:43\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W18-2501.pdf'\", \"'./acl_papers/W18-2501.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W18-2501.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m1.04\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:44\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W18-2501.pdf'\", \"'./acl_papers/W18-2501.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W18-2501.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.7\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m1.04\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:45\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for W18-2501: AllenNLP: A Deep Semantic Natural Language Processing Platform after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W18-2501.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:45\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for W19-5034: ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:46\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf'\", \"'./acl_papers/2020.findings-emnlp.58.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.36\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:46\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf'\", \"'./acl_papers/2020.findings-emnlp.58.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.65\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.36\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:47\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for 2020.findings-emnlp.58: Revisiting Pre-Trained Models for Chinese Natural Language Processing after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:48\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.acl-main.686.pdf'\", \"'./acl_papers/2020.acl-main.686.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.686.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.14\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:48\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/2020.acl-main.686.pdf'\", \"'./acl_papers/2020.acl-main.686.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.686.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.8\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.14\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:49\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for 2020.acl-main.686: HAT: Hardware-Aware Transformers for Efficient Natural Language Processing after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.686.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:49\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2022.emnlp-main.414: A Survey of Active Learning for Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:49\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W19-5006.pdf'\", \"'./acl_papers/W19-5006.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W19-5006.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.2\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:50\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W19-5006.pdf'\", \"'./acl_papers/W19-5006.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W19-5006.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.84\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.2\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:51\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for W19-5006: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W19-5006.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2021.emnlp-demo.21: Datasets: A Community Library for Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:52\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W17-1101.pdf'\", \"'./acl_papers/W17-1101.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W17-1101.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.77\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:53\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://www.aclweb.org/anthology/W17-1101.pdf'\", \"'./acl_papers/W17-1101.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W17-1101.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.34\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.77\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:53\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for W17-1101: A Survey on Hate Speech Detection using Natural Language Processing after 3 attempts. Error: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W17-1101.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:53\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf'\", \"'./acl_papers/Q18-1041.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('403 Client Error: Forbidden for url: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.42\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.0\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:54\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mstamina.retry_scheduled       \u001b[0m \u001b[36margs\u001b[0m=\u001b[35m(\"'https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf'\", \"'./acl_papers/Q18-1041.pdf'\")\u001b[0m \u001b[36mcallable\u001b[0m=\u001b[35m__main__.download_pdf\u001b[0m \u001b[36mcaused_by\u001b[0m=\u001b[35mHTTPError('403 Client Error: Forbidden for url: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf')\u001b[0m \u001b[36mkwargs\u001b[0m=\u001b[35m{}\u001b[0m \u001b[36mretry_num\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mwait_for\u001b[0m=\u001b[35m0.96\u001b[0m \u001b[36mwaited_so_far\u001b[0m=\u001b[35m0.42\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to download PDF for Q18-1041: Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science after 3 attempts. Error: 403 Client Error: Forbidden for url: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for P19-1159: Mitigating Gender Bias in Natural Language Processing: Literature Review\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mNo PDF available for 2020.aacl-main.46: A Survey of the State of Explainable AI for Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2021.eacl-demos.10: Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2021.naacl-main.201: A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios\u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m\n",
      "Summary:\n",
      "Fetched 8 PDFs.     \u001b[0m\n",
      "\u001b[2m2024-10-31 21:30:55\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mUnable to fetch 12 PDFs:\n",
      "J00-2011: Book Reviews: Foundations of Statistical Natural Language Processing\n",
      "2020.acl-demos.14: Stanza: A Python Natural Language Processing Toolkit for Many Human Languages\n",
      "D15-1075: A large annotated corpus for learning natural language inference\n",
      "J96-1002: A Maximum Entropy Approach to Natural Language Processing\n",
      "J00-4006: Book Reviews: Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition\n",
      "W18-2501: AllenNLP: A Deep Semantic Natural Language Processing Platform\n",
      "2020.findings-emnlp.58: Revisiting Pre-Trained Models for Chinese Natural Language Processing\n",
      "2020.acl-main.686: HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n",
      "W19-5006: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\n",
      "W17-1101: A Survey on Hate Speech Detection using Natural Language Processing\n",
      "Q18-1041: Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science\n",
      "2020.aacl-main.46: A Survey of the State of Explainable AI for Natural Language Processing\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "acl_papers = fetch_acl_papers(query=\"natural language processing\", max_results=20)\n",
    "get_pdfs_from_acl_id(acl_papers, output_dir=\"./acl_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from typing import Literal, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import stamina\n",
    "import structlog\n",
    "\n",
    "load_dotenv()\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "class Config(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query term, e.g., 'natural language processing'\")\n",
    "    venue: Literal[\"ACL Anthology\"] = Field(\"ACL Anthology\", description=\"Publication venue to filter papers from\")\n",
    "    num_papers: int = Field(10, gt=0, description=\"Number of papers to retrieve\")\n",
    "    output_dir: str = Field(\"acl_papers\", description=\"Directory to save downloaded PDFs\")\n",
    "    api_key: str | None = Field(None, description=\"Optional API key for Semantic Scholar\")\n",
    "    retry_attempts: int = Field(3, description=\"Number of retry attempts for downloading PDFs\")\n",
    "\n",
    "def fetch_acl_papers(query: str, max_results: int = 100) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches ACL papers based on a query, using pagination to retrieve more results if available.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search term to query papers.\n",
    "    - max_results (int): The maximum number of papers to fetch in total.\n",
    "    \n",
    "    Returns:\n",
    "    - dict[str, dict[str, Any]]: A dictionary with ACL IDs as keys and dictionaries containing\n",
    "      the title and paperId as values.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"fields\": \"paperId,title,externalIds,openAccessPdf\",\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
    "\n",
    "    acl_papers = {}\n",
    "    total_fetched = 0\n",
    "    token = None\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        if token:\n",
    "            params[\"token\"] = token\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Process and store ACL papers\n",
    "            for paper in data.get(\"data\", []):\n",
    "                external_ids = paper.get(\"externalIds\", {})\n",
    "                if \"ACL\" in external_ids:\n",
    "                    acl_id = external_ids[\"ACL\"]\n",
    "                    acl_papers[acl_id] = {\n",
    "                        \"title\": paper.get(\"title\"),\n",
    "                        \"paperId\": paper.get(\"paperId\"),\n",
    "                        \"openAccessPdf\": paper.get(\"openAccessPdf\", {}).get(\"url\") if paper.get(\"openAccessPdf\") else None\n",
    "                    }\n",
    "                    total_fetched += 1\n",
    "                    if total_fetched >= max_results:\n",
    "                        break\n",
    "\n",
    "            # Get the next token for pagination, or break if no more pages\n",
    "            token = data.get(\"token\")\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"Retrieved {len(acl_papers)} ACL papers.\")\n",
    "    return acl_papers\n",
    "\n",
    "\n",
    "@stamina.retry(on=requests.exceptions.RequestException, attempts=3)\n",
    "def download_pdf(pdf_url: str, pdf_filename: str) -> bool:\n",
    "    \"\"\"\n",
    "    Attempts to download a PDF file from the given URL and save it to the specified filename.\n",
    "    Retries up to 3 times on network-related exceptions.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_url (str): The URL of the PDF to download.\n",
    "    - pdf_filename (str): The path where the PDF will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if download is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(pdf_filename, \"wb\") as pdf_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            pdf_file.write(chunk)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pdfs_from_acl_id(acl_papers: dict[str, dict[str, Any]], output_dir: str = \"acl_papers\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads PDFs for the given ACL papers if a PDF URL is available, with retry capability,\n",
    "    and provides a summary of results.\n",
    "\n",
    "    Parameters:\n",
    "    - acl_papers (Dict[str, Dict[str, Any]]): Dictionary of ACL papers with ACL IDs as keys and metadata as values.\n",
    "    - output_dir (str): Directory to save downloaded PDFs.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Track successful and failed downloads\n",
    "    fetched_count = 0\n",
    "    unable_to_fetch = []\n",
    "\n",
    "    for acl_id, details in acl_papers.items():\n",
    "        pdf_url = details.get(\"openAccessPdf\")\n",
    "        \n",
    "        if pdf_url:\n",
    "            pdf_filename = os.path.join(output_dir, f\"{acl_id}.pdf\")\n",
    "            try:\n",
    "                success = download_pdf(pdf_url, pdf_filename)\n",
    "                if success:\n",
    "                    logger.info(f\"Downloaded PDF for {acl_id}: {details['title']}\")\n",
    "                    fetched_count += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Failed to download PDF for {acl_id}: {details['title']} after 3 attempts. Error: {e}\")\n",
    "                unable_to_fetch.append(f\"{acl_id}: {details['title']}\")\n",
    "        else:\n",
    "            logger.error(f\"No PDF available for {acl_id}: {details['title']}\")\n",
    "            unable_to_fetch.append(f\"{acl_id}: {details['title']}\")\n",
    "\n",
    "    # Summary of download results\n",
    "    logger.info(f\"\\nSummary:\\nFetched {fetched_count} PDFs.\\n\")\n",
    "    logger.warning(f\"Unable to fetch {len(unable_to_fetch)} PDFs:\\n\" + \"\\n\".join(unable_to_fetch))\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    \"\"\"\n",
    "    Main function to fetch and download ACL papers.\n",
    "    \"\"\"\n",
    "    # Set API key from config if provided\n",
    "    if config.api_key:\n",
    "        os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"] = config.api_key\n",
    "\n",
    "    # Fetch ACL papers\n",
    "    acl_papers = fetch_acl_papers(query=config.query, max_results=config.num_papers)\n",
    "\n",
    "    # Download PDFs for fetched papers\n",
    "    get_pdfs_from_acl_id(acl_papers, output_dir=config.output_dir)\n",
    "\n",
    "# Run the script using tyro to parse CLI arguments into Config class directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(query=\"vision language retrieval\" , num_papers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-31 21:45:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRetrieved 1 ACL papers.       \u001b[0m\n",
      "\u001b[2m2024-10-31 21:45:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDownloaded PDF for 2022.emnlp-main.488: mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections\u001b[0m\n",
      "\u001b[2m2024-10-31 21:45:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m\n",
      "Summary:\n",
      "Fetched 1 PDFs.\n",
      "    \u001b[0m\n",
      "\u001b[2m2024-10-31 21:45:51\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mUnable to fetch 0 PDFs:\n",
      "      \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Structured-Abstracts",
   "language": "python",
   "name": "structured-abstracts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
